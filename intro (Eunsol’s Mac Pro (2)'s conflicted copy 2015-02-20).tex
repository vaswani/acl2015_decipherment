\section{Introduction}
Tremendous advance in Machine Translation(MT) has been made since the introduction of parallel data and machine learning techniques. However, the reliance on parallel data also slows down development and application of high quality MT systems as the amount of parallel data is often limited for low density languages and various domains.

In general, it is easier to obtain comparable monolingual data. The ability to learn translations from monolingual data could alleviate obstacles caused by insufficient parallel data. Motivated by this idea, researchers have proposed different approaches to tackle this problem. 

The approaches to find translations from monolingual data can be largely divided into two groups. The first group is based on the idea proposed by \newcite{Rapp:1995}, where words are represented as context vectors, and two words are likely to be translations if their vectors are similar. Initially, the vectors contain just context words. Later, a number of work has extended this approach by introducing more features\cite{haghighi-EtAl:2008:ACLMain,Garera:2009,Bergsma:2011,Daume:2011:DAM:2002736.2002819,irvine-callisonburch:2013,irvine-callisonburch:2013:WMT}, using more abstract representation such as word embeddings\cite{KlementievCOLING}.

Another interesting approach to solve this problem is through decipherment. It has drawn significant amount of interests in the past few years\cite{ravi-knight:2011,Nuhn:2012,dou-knight:2013:EMNLP,ravi:2013}, and has been shown to improve machine translations. Decipherment views foreign languages as ciphers for English, and tries to find a translation table that converts foreign texts into sensible English. 

Both approaches have been shown to improve quality of MT systems for domain adaptation \cite{Daume:2011:DAM:2002736.2002819,Dou:2012,irvineQuirkDaumeEMNLP13} and low density languages \cite{irvine-callisonburch:2013:WMT,dou-vaswani-knight:2014:EMNLP2014}. Meanwhile, they also have their own advantages and disadvantages. While the first approach can take larger context into account, it requires high quality seed lexicons to learn a mapping between two vector spaces. In contrast, the second approach does not depend on seed lexicons, but is only able to look at limited context informed by either a bigram or trigram language model.  

In this work, we take advantages of both approaches by simultaneously performing decipherment and learning a mapping between two word vector space. More specifically, we extend previous work in large scale Bayesian decipherment by introducing a better base distribution, which is informed by mapping of word embedding vectors. The main contributions of this work are:

\begin{itemize}
\item We propose a new framework that combines two major approaches to find translations from monolingual data.

\item We show the new approach improves the state-of-the art decipherment accuracy by over two folds for multiple languages. 

\item We make our program a standard toolkit for finding translations from monolingual data for future research.
\end{itemize}