\section{Introduction}
Tremendous advance in Machine Translation(MT) has been made since we apply machine learning techniques to learn translation rules automatically from parallel data. However, the reliance on parallel data also limits development and application of high quality MT systems as the amount of parallel data is far from adequate for low density languages and various domains.

In general, it is easier to obtain comparable monolingual data. The ability to learn translations from monolingual data could alleviate obstacles caused by insufficient parallel data. Motivated by this idea, researchers have proposed different approaches to tackle this problem. 

The approaches to find translations from monolingual data can be largely divided into two groups. The first group is based on the idea proposed by \newcite{Rapp:1995}, where words are represented as context vectors, and two words are likely to be translations if their context vectors are similar. Initially, the vectors contain just context words. Subsequently, a number of work has extended this approach by introducing more features\cite{haghighi-EtAl:2008:ACLMain,Garera:2009,Bergsma:2011,Daume:2011:DAM:2002736.2002819,irvine-callisonburch:2013,irvine-callisonburch:2013:WMT}, and using more abstract representation such as word embeddings\cite{KlementievCOLING}.

Another promising approach to solve this problem is through decipherment. It has drawn significant amount of interests in the past few years\cite{ravi-knight:2011,Nuhn:2012,dou-knight:2013:EMNLP,ravi:2013}, and has been shown to improve machine translations. Decipherment views foreign languages as ciphers for English, and finds a translation table that converts foreign texts into sensible English. 

Both approaches have been shown to improve quality of MT systems for domain adaptation \cite{Daume:2011:DAM:2002736.2002819,Dou:2012,irvineQuirkDaumeEMNLP13} and low density languages \cite{irvine-callisonburch:2013:WMT,dou-vaswani-knight:2014:EMNLP2014}. Meanwhile, they also have their own advantages and disadvantages. While the former can take larger context into account, it requires high quality seed lexicons to learn a mapping between two vector spaces. In contrast, the latter does not depend on any seed lexicon, but is only able to look at limited context informed by either a bigram or trigram language model.  

In this work, we take advantages of both approaches and combine them in a joint inference process. More specifically, we extend previous work in large scale Bayesian decipherment by introducing a better base distribution, which is derived from similarities of word embedding vectors. The main contributions of this work are:

\begin{itemize}
\item We propose a new framework that combines two previous approaches that find translations from monolingual data.

\item We show the new approach improves the state-of-the art decipherment accuracy by over two folds for different language pairs(Spanish-English, Malagasy-English). 

\item We make our program that implements the idea in this work available for future research.
\end{itemize}