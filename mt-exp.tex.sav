\section{Improve Machine Translation with Decipherment}
The state-of-the-art machine translation systems are built with large amounts of parallel data. However, the amount of parallel data is limited for many language pairs and domains. In reality, we often have limited amount of out-of-domain parallel data (e.g. political) and large amounts of monolingual in-domain data (e.g. news), and we want to translate in-domain texts (news). In this section, we try to use a translation lexicon learned by deciphering large amounts of in-domain monolingual data to improve a phrase-based machine translation system trained with limited out-of-domain parallel data.

\subsection{Data}
We use Europarl corpus \cite{europarl} as our out-of-domain parallel training data. The Europarl corpus is extracted from the proceedings of the European Parliament and is in political domain. We choose the Spanish and English part of the corpus and limit the size of training data to 30k lines and approximately 1 million tokens for each language. Since we want to translate news texts, we use Gigaword as our in-domain monolingual training data to learn language models and a new translation lexicon to improve a phrase-based MT baseline system. For tuning and testing, we use the development data from the NAACL 2012 workshop on statistical machine translation. The data contains test data in the news domain from the 2008, 2009, 2010, and 2011 workshops. We use the 2008 test data as our tuning data and the rest as our testing data. The size of the training, tuning, and testing data is listed in Table \ref{data_size}. %Since our parallel training data is limited, we find that OOV words consists 8.7\% of the tuning and testing data.


 \begin{table}
 \begin{center}
 \begin{tabular}{ |c|c|c| } \hline
    \multicolumn{3}{|c|}{Parallel}  \\ \hline
    & Spanish & English  \\ \hline
   Europarl  & 1.1 million &   1.0 million  \\ \hline

   Tune-2008 & 52.6k & 49.8k  \\ \hline
   Test-2009 & 68.1k & 65.6k  \\ \hline
   Test-2010 & 65.5k & 61.9k  \\ \hline
   Test-2011 & 79.4k & 74.7k  \\ \hline
   \multicolumn{3}{|c|}{Non-Parallel}  \\ \hline
    & Spanish & English  \\ \hline
   Gigaword & 894 million &  940 million \\ \hline
 \end{tabular}

 \caption{Size of training, tuning, and testing data in number of tokens}
 \label{data_size}
 \end{center}
 \end{table}

\subsection{Baseline Machine Translation System}
We build a state-of-the-art phrase-based MT system, PBMT, using Moses \cite{Moses}.
PBMT has 3 models: a translation model, a distortion model, and a language model. We build a 5gram language model using the AFP section of the English Gigaword. We train the other models using the Europarl corpus. By default, Moses uses the following 8 features to score a candidate translation:

\begin{itemize}
\item direct and inverse translation probabilities
\item direct and inverse lexical weighting
\item a language model score
\item a distortion score
\item phrase penalty
\item word penalty
\end{itemize}

The 8 features have weights adjusted on the tuning data using minimum error rate training (MERT) \cite{Och:2003:MER:1075096.1075117}. PBMT has a phrase table $T_{phrase}$. During decoding, Moses copies out of vocabulary(OOV) words, which can not be found in $T_{phrase}$, directly to outputs. In the following sections, we describe how to use $T_{decipher}$ learned from large amounts of non-parallel data to improve translation of low frequency and OOV words.

\subsection{Decipherment for Machine Translation}
Experiment results in previous section show that using dependency bigrams for decipherment improves deciphering accuracy. However, the experiments are done with limited vocabulary. To improve MT, we make following changes to our sampling process to achieve better decipherment.

\begin{itemize}
\item Increase the size of ciphertext from 100 million tokens to 894 million tokens.
\item Increase the size of vocabulary from 5k to 50k.
\item Instead of seeding the sampling process randomly, we use a translation lexcion learned from limited amount of parallel data as seed.
\end{itemize}

We perform 20 random restarts with 10k iterations on each and build a word to word translation lexicon $T_{decipher}$ by collecting translation pairs seen in at least 3 final decipherments with either $P(f|e)\geq 0.2$ or $P(e|f)\geq 0.2$.


\subsection{Improving Translation of Observed Words with Decipherment}
To improve translation of words seen in parallel corpus, we simply  use $T_{decipher}$ as an additional parallel corpus. First, we filter $T_{decipher}$ by keeping only translation pairs $(f,e)$, where $f$ is observed in the Spanish part and $e$ is observed in the English part of the parallel corpus. Then we append all the Spanish words in the filtered $T_{decipher}$ to the end of Spanish part of the parallel corpus, and all the English words to the end of the English part of the parallel corpus. The training and tuning process is the same as the baseline machine translation system PBMT. We denote this system as Decipher-SEEN

%Since $T_{decipher}$ can be learned from deciphering either adjacent bigrams or dependency bigrams, we compare the following two systems against PBMT.

%\begin{itemize}
%\item Adj-Realign: Use $T_{decipher}$ learned from deciphering adjacent bigrams as additional parallel corpus for realignment.
%\item Dep-Realign: Use $T_{decipher}$ learned from deciphering typed dependency bigrams as additional parallel corpus for realignment.
%\end{itemize}

\subsection{Improving OOV translation with Decipherment}
\label{combine_tables}
Besides using $T_{decipher}$ to improve translation of low frequency words, we expect that $T_{decipher}$ contains a number of useful translations for OOV words. Our goal is to build a new system Decipher-OOV by integrating $T_{decipher}$ into the baseline system PBMT to help translate OOV words.

All systems in previous MT experiments have a phrase table $T_{phrase}$ learned from parallel data. By default, the decoder in Moses copies a source word $f$ not seen in $T_{phrase}$ directly to output. Through experiments, we find it is better to let those OOV words compete with their new translations suggested by $T_{decipher}$.  To implement this, we add a new translation pair $(f,f)$ for each source word $f$ in $T_{decipher}$ if the translation pair $(f,f)$ is not learned through decipherment. For each newly added translation pair, both of its log translation probabilities are set to $0$. To distinguish the added translation pairs from the others learned through decipherment, we add a binary feature $\theta$ to each translation pair in $T_{decipher}$. The final version of  $T_{decipher}$ has three feature scores: $P(e|f)$, $P(f|e)$, and $\theta$. Finally, we tune weights of the features in $T_{decipher}$ using MERT \cite{Och:2003:MER:1075096.1075117} on the tuning set.

Moses provides several ways to use multiple translation tables during decoding. We choose a backoff model, where during decoding, if a source $f$ is in $T_{phrase}$, its translation options are collected from $T_{phrase}$ exclusively. However, if  $f$ is not in $T_{phrase}$ but in $T_{decipher}$, the decoder will choose translation options from $T_{decipher}$. If $f$ is not in either of the translation table, the decoder just copies it directly to output.


%For the same reason stated previously, we compare the following two systems against PBMT.

%\begin{itemize}
%\item Adj-OOV: Use $T_{decipher}$ learned from deciphering adjacent bigrams for translating OOV words.
%\item Dep-OOV: Use $T_{decipher}$ learned from deciphering typed dependency bigrams for translating OOV words.
%\end{itemize}

In the end, we build a system Decipher-COMB, which uses $T_{decipher}$ learned from deciphering typed dependency bigrams to improve translation of both observed and OOV words.

\subsection{Results}
We tune both the baseline and the Decipher system 3 times with MERT and chooses the best weights based on BLEU scores on tuning set. The results are shown in Table \ref{result}.


 \begin{table*}[th]
 \begin{center}
 \begin{tabular}{ |c|c|c|c|c|c| } \hline
 System & $Tune_{2008}$ & $Test_{2009}$ & $Test_{2010}$ & $Test_{2011}$ \\ \hline
 PBMT(Baseline) &  19.1 & 19.6 & 21.30 & 22.1 \\ \hline \hline
 %Adj-Realign & Adjacent & 19.30 & 19.80 & 21.80  & 22.40 \\ \hline
 Decipher-SEEN & 19.7(19.5) & 20.5(20.1) & 22.5(22.2) & 23.0(22.6) \\ \hline \hline
% Adj-OOV & Adjacent & 19.10 & 19.70 & 21.50  & 22.20 \\ \hline
 Decipher-OOV &  19.9(19.4) & 20.4(19.9) & 22.4(21.7) & 22.9(22.5)\\ \hline \hline
 Decipher-COMB &  20.0(19.5) & 20.8() & 23.1() & 23.4(22.5) \\ \hline
 \end{tabular}
 \caption{Systems that use translation lexicons learned from decipherment show consistent improvement over the baseline system across tuning and testing sets. The best system, Decipher-Com, achieves as much as 1.8 BLEU point gain on the 2010 news test set. Numbers in parentheses are BLEU scores using a $T_{decipher}$ learned from deciphering adjacent bigrams.}
 \label{result}
 \end{center}
 \end{table*}

Results from Table \ref{result} show that both Adj-Realign and Dep-Realign achieve higher BLEU scores than PBMT across tuning and 3 test sets. Dep-Realign improves BLEU score by as high as 0.83 point on the 2011 news test set. We analyze the results and find the gain mainly comes from two parts. First, adding $T_{decipher}$ improves alignment; Second, $T_{decipher}$ also contains new alternative translations for words observed in the original parallel corpus.

Moreover, both Adj-OOV and Dep-OOV achieve better BLEU scores compared with PBMT across all tuning and test sets. We also observe that systems using $T_{decipher}$ learned by deciphering dependency bigrams leads to larger gains in BLEU scores. When decipherment is used to improve translation of both low frequency and OOV words, we observe improvement in BLEU score as high as 1.21 point on the 2011 news test set.

The consistent improvement on the tuning and different testing data suggests that decipherment is capable of learning good translations for a number of OOV words. To further demonstrate that our decipherment approach finds useful translations for OOV words, we list the top 10 most frequent OOV words from both the tuning set and testing set as well as their translations(up to three most likely translations) in Table \ref{oov_translation}. $P(e|f)$ and $P(f|e)$ are average scores over two or more different decipherments.

 \begin{table}
 \begin{center}
 \begin{tabular}{ |c|ccc|} \hline
Spanish & English & $P(e|f)$ & $P(f|e)$ \\ \hline
obama & his & 0.33 & 0.01 \\
           & bush & 0.27 & 0.07\\
           & clinton & 0.23 & 0.11 \\ \hline
bush & \textbf{bush} & 0.47 & 0.45 \\
        & yeltsin & 0.28 & 0.81 \\
        & he & 0.24 & 0.05 \\ \hline
festival & event & 0.68 & 0.35 \\
            &\textbf{festival} & 0.61 & 0.72 \\ \hline
wikileaks & zeta & 0.03 & 0.33 \\ \hline
venus & \textbf{venus} & 0.61 & 0.74 \\
          & serena & 0.47 & 0.62 \\ \hline
colchones & \textbf{mattresses} & 0.55 & 0.73 \\
                 & cars & 0.31 & 0.01 \\ \hline
helado & frigid & 0.52 & 0.44 \\
         & chill & 0.37 & 0.14 \\
           & sandwich & 0.42 & 0.27 \\ \hline
google & microsoft & 0.67 & 0.18 \\
            & \textbf{google} & 0.59 & 0.69 \\
cantante & singer & 0.44 & 0.92 \\
              & jackson & 0.14 & 0.33 \\
            & artists & 0.14 & 0.77 \\ \hline
mccain & \textbf{mccain} & 0.66 & 0.92 \\
            & it & 0.22 & 0.00 \\
            & he & 0.21 & 0.00 \\\hline
 \end{tabular}

 \caption{Decipherment finds correct translatons for 6 out of 10 most frequent OOV word types.}
 \label{oov_translation}
 \end{center}
 \end{table}

From the table, we can see that decipherment finds correct translations (bolded) for 6 out of the 10 most frequent OOV words. Although some translations obtained from decipherment are not correct, they are still semantically relevant. For instance, the words ``he'' and ``it'' are proper pronouns for ``obama'' and ``wikileaks'' respectively, ``google'' is a big online IT company, and ``candy'' is as sweet as ``helado'' (ice cream). 