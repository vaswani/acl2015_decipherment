\section{Decipherment Model}
In this section, we describe the decipherment model, upon which this work is built on. We first briefly introduce recent advances made in decipherment work, then describe the state-of-the-art approach, and in the end, bring up the problem that we address in this work. 

Unsupervised learning of translations from non-parallel is an old and challenging problem. In recent years, there has been growing interests in approaching it using decipherment techniques. \newcite{ravi-knight:2011} built an MT system using only non parallel data for translating movie subtitles. \newcite{Dou:2012} and \newcite{Nuhn:2012} made decipherment scalable to handle larger vocabulary. \newcite{dou-knight:2013:EMNLP} improved decipherment accuracy by using dependency relations between words. 

Throughout this paper, we use $f$ to denote target language or ciphertext tokens, and $e$ to denote source language or plaintext tokens. Given ciphertext $\mathbf{\cipher}:f_{1}...f_{n}$, the task of decipherment is to find a set of parameters $P(f_{i}|e_{i})$ that convert $F$ to sensible plaintext. The ciphertext $\mathbf{\cipher}$ can either be full sentences \cite{ravi-knight:2011,Nuhn:2012} or simply bigrams \cite{dou-knight:2013:EMNLP}. Since using bigrams and their counts speeds up decipherment, in this work, we treat $\mathbf{\cipher}$ as bigrams, where $ \mathbf{\cipher} = \{ \mathbf{\cipher}^n \}_{n=1}^{N} = \{ \cipher_1^n,\cipher_2^n \}_{n=1}^{N} $. 

Motivated by the idea from \newcite{Weaver:1955}, we model an observed cipher bigram $\mathbf{\cipher}^n$ with the following generative story:

\begin{itemize}
\item  First, a languae model $P(\mathbf{\plain})$ generates a sequence of two plaintext tokens $e_{1}^n,e_{2}^n$ with probability $P(e_{1}^n,e_{2}^n)$.
\item  Then, substitute $\plain_{1}^n$ with $\cipher_{1}^n$ and $\plain_{2}^n$ with $\cipher_{2}^n$ with probability $P(\cipher_{1}^n \mid e_{1}^n) \cdot P(f_{2}^n \mid e_{2}^n)$.
\end{itemize}

Based on the above generative story, the probability of any cipher bigram $\mathbf{\cipher^n}$ is:
%
\[
\label{p_cipher}
P(\mathbf{\cipher}^n) =  \sum_{e_{1} e_{2}} P(e_{1}e_{2}) \prod_{i=1}^{2}P(f_{i}^n \mid e_{i})
\]
%

Let the entire ciphertext corpus contains $N$ such bigrams $F_{1}...F_{N}$, we write down the probability of the ciphertext corpus as:
%
\[
\label{p_corpus}
P( \{ \mathbf{\cipher}^n \}_{n=1}^{N} ) =  \prod_{n=1}^{N} P(\mathbf{\cipher}^{n})
\]
%

There are two sets of parameters in the model: the channel probabilities, \{ $P(\cipher \mid \plain) \} $, and the bigram language model probabilities $\{ P(\plain' \mid \plain) \} $, where $\cipher$ ranges over the ciphertext vocabulary and $\plain,\plain'$ range over the plaintext vocabulary. Given a plaintext bigram language model, the training objective is to learn $P(\cipher \mid \plain)$ that maximize $P( \{ \mathbf{\cipher}^n \}_{n=1}^{N} )$. When formulated like this, one can directly apply EM to solve the problem \cite{knight-EtAl:2006}. However, EM has time complexity $O( N\cdot V_{e}^{2})$ and space complexity $O(V_{f}\cdot V_{e})$, where $V_{f}$, $V_{e}$ are the sizes of ciphertext and plaintext vocabularies respectively, and $N$ is the number of cipher bigrams. This makes the EM approach unable to handle long ciphertext with large vocabulary size. 
%Unfortunately, EM is not scalable when $V_{f}$, $V_{e}$, and $N$ are very large.
%$\cipher \in 1,\ldots,V_{\plain}$ and $\plain, \plain' \in 1,\ldots,V_e$

An alternative approach to solve the problem is Bayesian inference \cite{ravi-knight:2011,Dou:2012}. We assume that $P(\cipher \mid \plain)$ and $P(\plain' \mid \plain)$ are drawn from a Dirichet distribution with hyper-parameters $\alpha_{\cipher,\plain}$ and $\alpha_{\plain,\plain'}$, that is: 

\begin{align*}
P(\cipher \mid \plain) & \sim Dirichlet(\alpha_{\cipher,\plain}) \\ 
P(\plain \mid \plain') & \sim Dirichlet(\alpha_{\plain,\plain'}).
%P(C=0 \mid x) &= \frac{k q(x)}{\frac{n}Z p(x) + k q(x)} \\
%P(C=1 \mid x) &= \frac{\frac{n}Z p(x)}{\frac{n}Z p(x) + k q(x)}
\end{align*}

The remainder of the generative story is the same as the noisy channel model for decipherment. In the next section, we shall describe how we learn the hyper parameters of the Dirichlet prior. Given $\alpha_{\cipher,\plain}$ and $\alpha_{\plain,\plain'}$, The joint likelihood of the complete data and the parameters,
\begin{align} \label{joint_likelihood}
&P( \{ \mathbf{\cipher}^n , \mathbf{\plain}^n \}_{n=1}^{N}, \{ P(\cipher \mid \plain) \}, \{ P(\plain \mid \plain') \} )  \notag \\
 &= P( \{ \mathbf{\cipher}^n \mid \mathbf{\plain}^n \}_{n=1}^{N}, \{ P(\cipher \mid \plain) \}) \notag \\
     &P(  \{  \mathbf{\plain}^n \}_{n=1}^{N},P(\plain \mid \plain')) \notag \notag \\
 &= \prod_{\plain}  \frac{\dirgamma{\sum_{\cipher} \alpha_{\cipher,\plain}}} {\prod_{\cipher} \dirgamma{\alpha_{\plain,\cipher}}} \prod_{\cipher} P(\cipher \mid \plain)^{\#(\plain,\cipher)+\alpha_{\plain,\cipher} -1}  \notag \\
  &\prod_{\plain}  \frac{\dirgamma{\sum_{\plain'} \alpha_{\plain,\plain'}}} {\prod_{\plain'} \dirgamma{\alpha_{\plain,\plain'}}} \prod_{\cipher} P(\plain \mid \plain')^{\#(\plain,\plain')+\alpha_{\plain,\plain'} -1} , 
\end{align}

where $\#(\plain,\cipher)$ and $\#(\plain,\plain')$ are the counts of the translated word pairs and plaintext bigram pairs in the complete data, and $\dirgamma{\cdot}$ is the Gamma function. Unlike EM, in Bayeisan decipherment, we no longer search for parameters $P(\cipher \mid \plain)$ that maximize the likelihood of the observed ciphertext. Instead, we draw samples from posterior distribution of the plaintext sequences given the ciphertext. Under the above Bayesian decipherment model, it turns out  that the probability of a particular cipher word $\cipher_{j}$ having a value $k$, given the current plaintext word $\plain_j$, and  the samples for all the other ciphertext and plaintext words, $\mathbf{\cipher}_{-j}$ and $\mathbf{\plain}_{-j}$, is:

\begin{equation} \label{prob_bayes_ciphertext}
P(\cipher_j = k \mid \plain_j,\mathbf{\cipher}_{-j}, \mathbf{\plain}_{-j}) = \frac{\#(k, \plain_j)_{-j} + \alpha_{\plain_j,k}}{\#(\plain_j)_{-j}+\sum_{\cipher} \alpha_{\plain_j,\cipher}}.
\end{equation}

Where, $\#(k, \plain_j)_{-j}$ and $\#(\plain_j)_{-j}$ are the counts of the ciphertext, plaintext word pair and plaintext word in the samples excluding $\cipher_j$ and $\plain_j$. Similarly, the probability of a plaintext word $\plain_j$ taking a value $l$ given samples for all other plaintext words, 
\begin{equation} \label{prob_bayes_plaintext}
P(\plain_j = l \mid \mathbf{\plain}_{-j}) = \frac{\#(l, \plain_{j-1})_{-j} + \alpha_{l,\plain_{j-1}}} {\#(\plain_{j-1})_{-j} + \sum_{\plain} \alpha_{\plain,\plain_{j-1}}}.
\end{equation}


%\begin{equation}
%\end{equation}

Since we have large amounts of plaintext data, we can train a high quality bigram language model, $P_{LM}(\plain \mid \plain')$ and use it to guide our samples and learn a better posterior distribution. For that, we define $\alpha_{\plain,\plain'} = \alpha P_{LM}(\plain \mid \plain')$, and set $\alpha$ to be very high. The probability of a plaintext word (Equation~\ref{prob_bayes_plaintext}) is now

\begin{equation} \label{prob_bayes_plaintext_lm}
P(\plain_j = l \mid \mathbf{\plain}_{-j}) \approx P_{LM}(l \mid \plain_{j-1}).
\end{equation}

\marginpar{We have to say that in previous work , $\alpha_{\plain,\cipher}$ has been set to $\alpha * uniform$}. To sample from the posterior, we iterate over the observed ciphertext bigram tokens and use equations~\ref{prob_bayes_ciphertext} and~\ref{prob_bayes_plaintext_lm} to sample a plaintext token with probability

\begin{align} \label{prob_sampling_plaintext}
P( \plain_j \mid \mathbf{\plain}_{-j}, \mathbf{\cipher} ) \propto & P_{LM}(\plain_j \mid \plain_{j-1}) \times \\
                                                                                                & P(\cipher_j \mid \plain_j,\mathbf{\cipher}_{-j}, \mathbf{\plain}_{-j}).
\end{align}

\iffalse
%
\[
\label{p_sample}
P_{sample}(e_{1}e_{2}) =  P(e_{1}e_{2}) \prod_{i=1}^{2}P_{CRP}(f_{i}|e_{i})
\]
%
In the above equation, the translation probability $P_{CRP}(f_{i}|e_{i})$ is modeled by Chinese Restaurant Process(CRP), and is defined in Equation \ref{p_channel}.
%
\[
\label{p_channel}
P_{CRP}(f_{i}|e_{i}) = \frac{\alpha P_0(f_{i}|e_{i})+count(f_{i},e_{i})}{\alpha+count(e_{i})}
\]
%
where $P_{0}$ is a base distribution, also known as a prior, and $\alpha$ is a parameter that controls how much we trust the base distribution. $count(f_{i},e_{i})$ and $count(e_{i})$ record the number of times $f_{i},e_{i}$ and $e_{i}$ appear in previously generated samples respectively. The base distribution is given independently, and in all the previous work, it is set to uniform.
\fi

At the end of sampling, we compute $P(\cipher \mid \plain)$ from ciphertext and its plaintext samples using maximum likelihood estimation:

\[
\label{mlh_estimation}
P(\cipher \mid \plain) =  \frac{\#(\cipher,\plain)}{\#(\plain)}.
\]

In the next section, we will describe how we model and learn an appropriate base distribution for $P(\cipher \mid \plain)$.


